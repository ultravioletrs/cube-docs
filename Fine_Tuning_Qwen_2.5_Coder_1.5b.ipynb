{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Upgrade PyTorch to 2.5.0\n",
    "!pip install torch==2.5.0+cu121 -f https://download.pytorch.org/whl/cu121/torch_stable.html\n",
    "\n",
    "# Install Unsloth, Xformers (Flash Attention) and all other packages!\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install --no-deps xformers \"trl<0.9.0\" peft accelerate bitsandbytes\n",
    "\n",
    "# Install necessary packages\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Set Hugging Face token\n",
    "HF_TOKEN = 'hf_token'\n",
    "os.environ['HF_TOKEN_READ'] = HF_TOKEN\n",
    "\n",
    "# Define model parameters\n",
    "max_sequence_length = 2048\n",
    "model_dtype = None\n",
    "load_in_4bit_mode = True\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"Qwen/Qwen2.5-Coder-1.5B\",\n",
    "    max_seq_length=max_sequence_length,\n",
    "    dtype=model_dtype,\n",
    "    load_in_4bit=load_in_4bit_mode,\n",
    "    token=os.environ.get('HF_TOKEN_READ'),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Mount Google Drive to access and save files\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Define paths for input and output files on Google Drive\n",
    "input_file_path = '/content/drive/MyDrive/fine_tune_data.json'  # Path to the preprocessed fine-tuning data\n",
    "output_file_path = '/content/drive/MyDrive/formatted_dataset.jsonl'  # Output JSONL file on Google Drive\n",
    "\n",
    "# Import necessary libraries\n",
    "import json\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the tokenizer and set the EOS token\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-Coder-1.5B\")  # Replace with your actual model\n",
    "end_of_sequence_token = tokenizer.eos_token\n",
    "\n",
    "# Load the fine-tuning data JSON file from Google Drive\n",
    "with open(input_file_path, 'r', encoding='utf-8') as file:\n",
    "    fine_tune_data = json.load(file)\n",
    "\n",
    "# Convert the data to a Hugging Face Dataset\n",
    "dataset = Dataset.from_list(fine_tune_data)\n",
    "\n",
    "# Define the formatting template for combining fields into a single text\n",
    "formatted_prompt_template = \"\"\"Below is an instruction, an input, and a response that completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "# Function to format each prompt-input-completion entry\n",
    "def format_prompt_input_completion_pairs(examples):\n",
    "    prompts = examples[\"prompt\"]\n",
    "    inputs = examples[\"input\"]\n",
    "    completions = examples[\"completion\"]\n",
    "\n",
    "    # Combine prompt, input, and completion with EOS token\n",
    "    combined_texts = [\n",
    "        formatted_prompt_template.format(prompt, input_text, completion) + end_of_sequence_token\n",
    "        for prompt, input_text, completion in zip(prompts, inputs, completions)\n",
    "    ]\n",
    "    return {\"text\": combined_texts}\n",
    "\n",
    "# Apply formatting to each entry in the dataset\n",
    "formatted_dataset = dataset.map(format_prompt_input_completion_pairs, batched=True)\n",
    "\n",
    "# Save the formatted dataset to JSONL in Google Drive, overwriting if it exists\n",
    "with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "    for entry in formatted_dataset:\n",
    "        json.dump({\"text\": entry[\"text\"]}, output_file)\n",
    "        output_file.write('\\n')\n",
    "\n",
    "print(f\"Formatted dataset saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Add LoRA adapters\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,  # Lower rank reduces model capacity, helping avoid overfitting\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha=8,  # Lower alpha can prevent overfitting\n",
    "    lora_dropout=0.1,  # Small dropout to improve regularization\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",  # Helps if long context lengths are needed\n",
    "    random_state=3407,\n",
    "    use_rslora=False,\n",
    "    loftq_config=None,\n",
    ")\n",
    "\n",
    "# Train and save the model\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "# Use formatted_dataset which contains the text column\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=formatted_dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_sequence_length,\n",
    "    dataset_num_proc=2,\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=1,  # Lower batch size for small datasets\n",
    "        gradient_accumulation_steps=8,  # Increase accumulation to maintain larger effective batch size\n",
    "        num_train_epochs=2,  # Use more epochs to make the most of limited data\n",
    "        warmup_ratio=0.1,  # Start with a small ratio to avoid large initial steps\n",
    "        learning_rate=5e-5,  # Lower learning rate to prevent overfitting\n",
    "        fp16=not is_bfloat16_supported(),\n",
    "        bf16=is_bfloat16_supported(),\n",
    "        logging_steps=1,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.0,  # Set to zero to avoid over-penalizing limited data\n",
    "        lr_scheduler_type=\"cosine\",  # Smoothly decays learning rate\n",
    "        seed=3407,\n",
    "        output_dir=\"outputs\",\n",
    "        report_to=\"none\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!curl -fsSL https://ollama.com/install.sh | sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%cd llama.cpp\n",
    "!make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Save the model in all available GGUF quantization formats and upload to Hugging Face Hub\n",
    "model.push_to_hub_gguf(\n",
    "    \"hf_username/model_name\",\n",
    "    tokenizer=tokenizer,\n",
    "    quantization_method=[\"q4_k_m\", \"q5_k_m\", \"q8_0\", \"f16\"],\n",
    "    token=\"hf_token\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
