{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Cube AI","text":"<p>Cube AI is a framework for building GPT-based AI applications using confidential computing. It protects user data and the AI model by using a trusted execution environment (TEE). TEE is a secure area of a processor that ensures that code and data loaded inside it are protected with respect to confidentiality and integrity. Data confidentiality prevents unauthorized access of data from outside the TEE, while code integrity ensures that code inside the TEE remains unchanged and unaltered from unauthorized access.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Secure Computing: Cube AI uses secure enclaves to protect user data and AI models from unauthorized access.</li> <li>Trusted Execution Environment (TEE): Cube AI uses a trusted execution environment to ensure that AI models are executed securely and in a controlled environment.</li> <li>Scalability: Cube AI can handle large amounts of data and AI models, making it suitable for applications that require high performance and scalability.</li> <li>Multiple LLM Backend Support: Supports both Ollama and vLLM for flexible model deployment and high-performance inference.</li> <li>OpenAI-Compatible API: Provides familiar API endpoints for easy integration with existing applications.</li> </ul>"},{"location":"#supported-llm-backends","title":"Supported LLM Backends","text":""},{"location":"#vllm-integration","title":"vLLM Integration","text":"<p>Cube AI now supports vLLM, a high-throughput and memory-efficient inference engine for Large Language Models. vLLM provides:</p> <ul> <li>High Throughput: Optimized for serving multiple concurrent requests with continuous batching</li> <li>Memory Efficiency: Advanced memory management techniques for large models</li> <li>Fast Inference: Optimized CUDA kernels and efficient attention mechanisms</li> <li>Model Compatibility: Supports popular architectures including LLaMA, Mistral, Qwen, and more</li> </ul>"},{"location":"#ollama-integration","title":"Ollama Integration","text":"<p>Cube AI also integrates with Ollama for local model deployment, providing:</p> <ul> <li>Easy model management and deployment</li> <li>Local inference capabilities</li> <li>Support for various open-source models</li> </ul>"},{"location":"#why-cube-ai","title":"Why Cube AI?","text":"<p>Traditional GPT-based AI applications often rely on public cloud services, which can be vulnerable to security breaches and unauthorized access. The tenant for example openai, and the hardware provider for example Azure, are not always transparent about their security practices and can be easily compromised. They can also access your prompts and model responses. Cube AI addresses these privacy concerns by using TEEs. Using TEEs, Cube AI ensures that user data and AI models are protected from unauthorized access outside the TEE. This helps to maintain user privacy and ensures that AI models are used in a controlled and secure manner.</p>"},{"location":"#how-does-cube-ai-work","title":"How does Cube AI work?","text":"<p>Cube AI uses TEEs to protect user data and AI models from unauthorized access. TEE offers an execution space that provides a higher level of security for trusted applications running on the device. In Cube AI, the TEE ensures that AI models are executed securely and in a controlled environment.</p>"},{"location":"architecture/","title":"Architecture","text":"<p>Cube AI is built on top of SuperMQ, Ollama and a custom proxy server. All these host the Cube AI API that hosts the AI models and protects prompts and user data securely.</p> <p></p>"},{"location":"architecture/#supermq","title":"SuperMQ","text":"<p>Cube AI uses SuperMQ Users and Auth Service to manage users and authentication. SuperMQ is an IoT platform that provides a secure and scalable way to manage IoT devices. Since SuperMQ is based on micro-service architecture, auth and users are separated from the core application. This allows for better scalability and security. SuperMQ is responsible for users authentication, authorization and user management. It also provides a secure way to store user data.</p>"},{"location":"architecture/#ollama","title":"Ollama","text":"<p>Ollama is a framework that provides a unified interface for interacting with different LLMs. Ollama is responsible for managing LLMs and their configurations. It provides a unified interface for interacting with different LLMs, allowing developers to easily switch between different LLMs without having to change their code. Ollama also provides a way to manage LLMs, including configuring LLMs, managing prompts, and managing models.</p>"},{"location":"architecture/#proxy-server","title":"Proxy Server","text":"<p>The proxy server is responsible for handling requests to ollama. Once a user is registered on SuperMQ and issued with an access token, the user can interact with Cube AI using the issued token. The proxy server will verify the token and ensure that the user has the necessary permissions to access the Cube AI API by checking it with the SuperMQ Auth Service. Once the request is verified, the proxy server will forward the request to the appropriate Ollama instance. The proxy server also handles authentication and authorization for the user. It ensures that only authorized users can access the Cube AI API.</p>"},{"location":"attestation/","title":"Attestation","text":"<p>Attestation is a process of verifying the integrity and authenticity of software or hardware components. In the context of Cube AI, attestation is used to verify the integrity of the Cube AI API and the Ollama instances.</p>"},{"location":"attestation/#runtime-encryption","title":"Runtime Encryption","text":"<p>Cube AI runs inside AMD SEV-SNP based confidential virtual machines (CVMs). The CVMs are secured using AMD SEV-SNP, which provides a secure enclave for running applications. With SEV-SNP, the memory of the CVM is encrypted, ensuring that the data is not accessible to the host operating system or other applications running on the host. This ensures that the data is not accessible to unauthorized users. The encryption keys are generated using a hardware-based key management system, ensuring that the keys are not accessible to unauthorized users. Even if an attacker gains access to the hypervisor, they will not be able to read the encrypted data or access the encryption keys.</p>"},{"location":"developer-guide/","title":"Developer Guide","text":"<p>This guide will help you get started with developing, testing, deploying, and running Cube AI.</p>"},{"location":"developer-guide/#cloning-the-repository","title":"Cloning the Repository","text":"<p>Clone Cube AI repository by running:</p> <pre><code>git clone https://github.com/ultravioletrs/cube.git\ncd cube\n</code></pre>"},{"location":"developer-guide/#building-and-running","title":"Building and Running","text":""},{"location":"developer-guide/#building-docker-images","title":"Building Docker Images","text":"<p>You can build the Docker images for Cube AI and related services using the <code>make</code> command in the project's root directory.</p> <p>To build the production Docker image, use:</p> <pre><code>make docker\n</code></pre> <p>For the development Docker image, use:</p> <pre><code>make docker-dev\n</code></pre>"},{"location":"developer-guide/#running-services","title":"Running Services","text":"<p>To start the Cube AI services, simply run:</p> <pre><code>make up\n</code></pre> <p>This command will start the services using the default configuration (Ollama backend).</p>"},{"location":"developer-guide/#configuration","title":"Configuration","text":"<p>You can configure the services using environment variables or by modifying the <code>docker/.env</code> file.</p> <p>To use the vLLM backend:</p> <pre><code>make up-vllm\n</code></pre> <p>To use the Ollama backend (default):</p> <pre><code>make up-ollama\n</code></pre> <p>For custom port configurations, update the <code>docker/.env</code> file:</p> <pre><code>UI_PORT=8080\nUV_CUBE_NEXTAUTH_URL=http://&lt;your-ip&gt;:${UI_PORT}\n</code></pre> <p>Then restart the services:</p> <pre><code>make restart\n</code></pre> <p>For a full list of available commands, run:</p> <pre><code>make help\n</code></pre> <p>After successfully deploying Cube AI, you can perform various tasks to set up and manage your instance. These include:</p> <ul> <li>Managing domains</li> <li>Managing users</li> <li>Generating authentication tokens (using curl command)</li> <li>Intergrating your <code>Cube AI</code> instance with <code>Continue</code> Visual Studio Code extension to leverage large language models (LLMs) directly within VS Code to assist with coding task</li> </ul> <p>For detailed instructions on the above tasks and more, refer to the Getting Started Guide.</p>"},{"location":"developer-guide/#open-web-ui-integration","title":"Open Web UI Integration","text":"<p>Open Web UI is integrated into Cube AI to help in debugging and monitoring key performance metrics of the LLMs, including response token/s, prompt token/s, total duration, and load duration. For more detailed setup and configuration instructions, refer to the Open Web UI documentation.</p> <p>To access Open Web UI, once Cube AI services are up and running, open your browser and navigate to:</p> <pre><code>http://&lt;your-server-ip-address&gt;:3000\n</code></pre> <p>While it should work out of the box, occasionally, when you submit a prompt through the Open Web UI, you might encounter an error like this:</p> <pre><code>Ollama: 400, message='Bad Request', url='http://ollama:11434/api/chat'\n</code></pre> <p>To resolve the error:</p> <ul> <li>Click on your profile icon in the top-right corner of the Open Web UI interface.</li> <li>Navigate to Settings.</li> <li>Select Admin Settings.</li> <li>In the Admin Panel, select Connections from the sidebar.</li> <li>Under the Ollama API section, click the refresh icon next to the Ollama API URL (<code>http://ollama:11434</code>).</li> <li>After refreshing, you should see a confirmation message stating \"Server connection verified\". This should reset the connection to the Ollama service and resolve the \"Bad Request\" error.</li> </ul>"},{"location":"developer-guide/#hardware-abstraction-layer-hal-for-confidential-computing","title":"Hardware Abstraction Layer (HAL) for Confidential Computing","text":"<p>For detailed instructions on setting up and building Cube HAL, please refer to this guide. It covers:</p> <ul> <li>Cloning the Buildroot and Cube repositories</li> <li>Configuring and building Cube HAL</li> <li>Running Cube HAL in a virtual machine</li> </ul>"},{"location":"developer-guide/#private-model-upload","title":"Private Model Upload","text":"<ol> <li>Package the Model Files</li> </ol> <p>First, compress your model files into a <code>.tar.gz</code> archive to prepare them for transfer:</p> <p><code>bash    tar -czvf model-name.tar.gz /path/to/model/files</code></p> <ol> <li>Set Up Cube HAL</li> </ol> <p>Follow the Hardware Abstraction Layer (HAL) for Confidential Computing setup to:</p> <ul> <li>Install Buildroot</li> <li>Clone the Cube AI repository</li> <li> <p>Pull and start Cube AI docker containers</p> </li> <li> <p>Enable SSH and Access the CVM</p> </li> </ul> <p>SSH is needed to securely connect and transfer files to the CVM. Follow the steps in the SSH guide to enable SSH access on the CVM.</p> <ol> <li>Transfer the Model Archive to the CVM</li> </ol> <p>Once SSH is set up, use <code>scp</code> to securely copy the <code>.tar.gz</code> file from step 1 to the CVM. For detailed steps, refer to the SSH guide.</p> <ol> <li>Decompress and Extract the Model Files on the CVM    After transferring the archive to the CVM, log in to the CVM and use <code>gunzip</code> to decompress the <code>.tar.gz</code> file, then extract the <code>.tar</code> file:</li> </ol> <p><code>bash    gunzip model-name.tar.gz    tar -xvf model-name.tar</code></p> <ol> <li>Copy the Extracted Model Files to the <code>ollama</code> Container</li> </ol> <p>With the model files unpacked, copy them to the <code>ollama</code> container running on the CVM:</p> <p><code>bash    docker cp /path/to/extracted/files ollama:/path/to/target/directory/in/container</code></p> <p>Replace <code>/path/to/extracted/files</code> with the path of the unpacked files on the CVM and <code>/path/to/target/directory/in/container</code> with the destination directory inside the <code>ollama</code> container.</p>"},{"location":"developer-guide/#fine-tuning-base-model-on-custom-code-dataset","title":"Fine-Tuning Base Model on Custom Code Dataset","text":"<p>To enhance the performance of the base model on domain-specific tasks or particular code styles, you may want to fine-tune it on a custom code dataset. This process will help adapt the model to your specific requirements.</p>"},{"location":"developer-guide/#prerequisites","title":"Prerequisites","text":"<ol> <li> <p>Python Environment: Ensure you have Python 3.8 or above.</p> </li> <li> <p>Dependencies:</p> </li> <li> <p><code>transformers</code>: for model handling. For more information, see the transformers Documentation.</p> </li> <li><code>datasets</code>: for data handling. For more information, see the datasets library.</li> <li><code>torch</code>: for model training. Alternative is <code>TensorFlow</code> with <code>Keras</code>. For more information, see the TensorFlow Documentation.</li> <li><code>peft</code>: for parameter-efficient fine-tuning (PEFT) if needed. For more information, see the PEFT Documentation.</li> <li><code>unsloth</code>: for advanced model handling and integration of LoRA adapters and other fine-tuning techniques. For more information, see the unsloth GitHub Repository.</li> <li>Optional Libraries: Install additional packages for LoRA fine-tuning, data handling, and model quantization on Google Colab. Execute the following in Colab:</li> </ol> <p><code>python    !pip install torch==2.5.0+cu121 -f https://download.pytorch.org/whl/cu121/torch_stable.html    !pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"    !pip install --no-deps xformers \"trl&lt;0.9.0\" peft accelerate bitsandbytes</code></p> <ol> <li> <p>Code Database: You can use a public code dataset from Hugging Face\u2019s datasets library or your own code dataset.</p> </li> <li> <p>Google Drive (Optional): Mount Google Drive if you're using Colab and need a place to store datasets and model outputs.</p> </li> </ol> <p><code>python    from google.colab import drive    drive.mount('/content/drive')</code></p> <ol> <li>Ollama: Install Ollama for model deployment.</li> </ol> <p><code>bash    !curl -fsSL https://ollama.com/install.sh | sh</code></p>"},{"location":"developer-guide/#step-1-prepare-your-code-dataset","title":"Step 1: Prepare Your Code Dataset","text":"<p>Organize your dataset into prompt-input-completion pairs:</p> <ul> <li>Prompt: The task or question, e.g., \"Write a function to reverse a string.\"</li> <li>Input: Additional context to guide the model, like sample data or requirements.</li> <li>Completion: The expected solution, such as the completed function code.</li> </ul> <p>This structure enables effective model learning by linking each prompt and input to the correct completion. For more details, refer to the TRL Dataset Formats Documentation.</p> <pre><code>from datasets import load_dataset\nfrom transformers import AutoTokenizer\n\n# Load your dataset - replace `your_code_dataset` with your dataset name or path to your file\n# Example: If using a custom CSV file, specify the path as shown below.\ndataset = load_dataset(\"csv\", data_files=\"path/to/your/data.csv\")\n\n# Initialize tokenizer (replace with your specific model)\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-Coder-1.5B\")\n\n# Define a structured template for formatting\nformatted_prompt_template = \"\"\"Below is an instruction, an input, and a response that completes the request.\n\n### Instruction:\n{}\n\n### Input:\n{}\n\n### Response:\n{}\"\"\"\n\n# Define a function to format each example in prompt-input-completion structure\ndef format_prompt_input_completion_pairs(examples):\n    combined_texts = [\n        formatted_prompt_template.format(prompt, input_text, completion) + tokenizer.eos_token\n        for prompt, input_text, completion in zip(examples[\"prompt\"], examples[\"input\"], examples[\"completion\"])\n    ]\n    return {\"text\": combined_texts}\n\n# Apply the formatting function to the dataset in batches for efficiency\nformatted_dataset = dataset.map(format_prompt_input_completion_pairs, batched=True)\n</code></pre> <p>If your code dataset has several files like in a github repository, first convert them into a single text file to streamline preprocessing. To do this, use Repomix. With the generated <code>repomix-output.txt</code>, create prompt-input-completion pairs by segmenting each function or code block in line with your chosen prompt.</p> <p>Save the formatted dataset if working on Google Colab with Google Drive:</p> <pre><code>output_file_path = '/content/drive/MyDrive/formatted_dataset.jsonl'\nwith open(output_file_path, 'w', encoding='utf-8') as output_file:\n    for entry in formatted_dataset:\n        json.dump({\"text\": entry[\"text\"]}, output_file)\n        output_file.write('\\n')\n</code></pre>"},{"location":"developer-guide/#step-3-add-lora-adapters-optional-for-parameter-efficient-fine-tuning","title":"Step 3: Add LoRA Adapters (Optional for Parameter-Efficient Fine-Tuning)","text":"<p>LoRA (Low-Rank Adaptation) adapters help reduce memory usage and improve training efficiency by modifying specific layers of the model.</p> <pre><code>from unsloth import FastLanguageModel\n\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r=16,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n    lora_alpha=8,\n    lora_dropout=0.1,\n    bias=\"none\",\n    use_gradient_checkpointing=\"unsloth\",\n    random_state=3407\n)\n</code></pre>"},{"location":"developer-guide/#step-4-define-training-arguments","title":"Step 4: Define Training Arguments","text":"<p>Set up your training arguments for fine-tuning. Adjust the following values based on the dataset size, available hardware, and specific requirements.</p> <pre><code>training_args = TrainingArguments(\n    output_dir=\"./qwen_coder_finetuned\",\n    evaluation_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2,\n    num_train_epochs=3,\n    weight_decay=0.01,\n    save_total_limit=2,\n    push_to_hub=False,  # Set to True if pushing to Hugging Face hub\n)\n</code></pre> <p>For PEFT models using LoRA, you might want to adjust batch sizes, accumulation steps, and number of epochs to better suit smaller datasets.</p> <pre><code>training_args = TrainingArguments(\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=8,\n    num_train_epochs=2,\n    learning_rate=5e-5,\n    fp16=not is_bfloat16_supported(),\n    bf16=is_bfloat16_supported(),\n    logging_steps=1,\n    optim=\"adamw_8bit\",\n    output_dir=\"outputs\",\n)\n</code></pre>"},{"location":"developer-guide/#step-5-initialize-the-sft-trainer","title":"Step 5: Initialize the SFT Trainer","text":"<p>Use the Hugging Face <code>Trainer</code> to manage the training process.</p> <pre><code>trainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"validation\"],\n)\n</code></pre> <p>For LoRA fine-tuning, use <code>SFTTrainer</code> instead:</p> <pre><code>from trl import SFTTrainer\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=formatted_dataset,\n    dataset_text_field=\"text\",\n    max_seq_length=2048,\n)\n</code></pre>"},{"location":"developer-guide/#step-6-train-the-model","title":"Step 6: Train the Model","text":"<p>Start the fine-tuning process. This may take some time depending on your dataset size and compute resources.</p> <pre><code>trainer.train()\n</code></pre>"},{"location":"developer-guide/#step-7-evaluate-the-model-optional","title":"Step 7: Evaluate the Model (Optional)","text":"<p>After training, evaluate the model on a test dataset to check its performance.</p> <pre><code>eval_results = trainer.evaluate()\nprint(f\"Evaluation results: {eval_results}\")\n</code></pre>"},{"location":"developer-guide/#step-8-save-and-export-the-model","title":"Step 8: Save and Export the Model","text":"<p>Save the fine-tuned model to the local disk, or optionally push it to the Hugging Face Hub.</p> <pre><code>trainer.save_model(\"path/to/save/qwen_coder_finetuned\")\n# Optionally, push to Hugging Face Hub if push_to_hub=True in TrainingArguments\n# trainer.push_to_hub()\n</code></pre>"},{"location":"developer-guide/#step-9-convert-to-gguf-format-using-llamacpp","title":"Step 9: Convert to GGUF Format Using <code>llama.cpp</code>","text":"<p>To make the fine-tuned model deployable on a variety of platforms, you can convert it to GGUF formats using <code>llama.cpp</code>.</p> <pre><code>    %cd llama.cpp\n    !make\n    model.push_to_hub_gguf(\n        \"hf_username/model_name\",\n        tokenizer=tokenizer,\n        quantization_method=[\"q4_k_m\", \"q5_k_m\", \"q8_0\", \"f16\"],\n        token=\"hf_token\"\n    )\n</code></pre> <p>For a complete notebook fine-tuning Qwen 2.5 Coder 1.5B, refer to Fine-Tuning Qwen 2.5 Coder 1.5B</p>"},{"location":"developer-guide/#cleaning-up-your-dockerized-cube-ai-setup","title":"Cleaning up your Dockerized Cube AI Setup","text":"<p>If you want to stop and remove the Cube AI services, volumes, and networks created during the setup, follow these steps:</p> <p>First, stop all running containers:</p> <pre><code>docker compose down\n</code></pre> <p>Remove volumes and networks:</p> <pre><code>docker compose down --volumes --remove-orphans\n</code></pre> <p>To clean up the build artifacts and remove compiled files, use:</p> <pre><code>make clean\n</code></pre>"},{"location":"getting-started/","title":"Getting Started","text":""},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker and Docker Compose</li> <li>NVIDIA GPU with CUDA support (recommended for vLLM)</li> <li>Hardware with TEE support (AMD SEV-SNP or Intel TDX)</li> </ul>"},{"location":"getting-started/#quick-start","title":"Quick Start","text":"<ol> <li>Clone the repository</li> </ol> <p><code>bash    git clone https://github.com/ultravioletrs/cube.git    cd cube</code></p> <ol> <li>Start Cube AI services</li> </ol> <p><code>bash    make up</code></p> <ol> <li>Configuration    Cube AI can be configured to use different backends. The default backend is Ollama.</li> </ol> <p>Ollama:    <code>bash     make up-ollama</code></p> <p>vLLM:    <code>bash     make up-vllm</code></p> <ol> <li>Get your authentication token</li> </ol> <p>All API requests require authentication using JWT tokens. Once the services are running, obtain a JWT token:</p> <p><code>bash    curl -ksSiX POST https://localhost/users/tokens/issue \\      -H \"Content-Type: application/json\" \\      -d '{        \"username\": \"admin@example.com\",        \"password\": \"m2N2Lfno\"      }'</code></p> <p>The response will contain your JWT token:</p> <p><code>json    {      \"access_token\": \"eyJhbGciOiJIUzUxMiIsInR5cCI6IkpXVCJ9...\",      \"refresh_token\": \"...\"    }</code></p> <ol> <li>Create a domain</li> </ol> <p>All API requests require a domain ID in the URL path. You can either get the domain ID from the UI or create a new domain via the API:</p> <p><code>bash    curl -sSiX POST http://localhost:9003/domains \\      -H \"Content-Type: application/json\" \\      -H \"Authorization: Bearer YOUR_ACCESS_TOKEN\" \\      -d '{        \"name\": \"Magistrala\",        \"route\": \"magistrala\",        \"tags\": [\"absmach\", \"IoT\"],        \"metadata\": {          \"region\": \"EU\"        }      }'</code></p> <p>The response will contain your domain information including the <code>id</code>:</p> <p><code>json    {      \"id\": \"d7f9b3b8-4f7e-4f44-8d47-1a6e5e6f7a2b\",      \"name\": \"Magistrala\",      \"route\": \"magistrala\",      \"tags\": [\"absmach\", \"IoT\"],      \"metadata\": {        \"region\": \"EU\"      },      \"status\": \"enabled\",      \"created_by\": \"c8c3e4f1-56b2-4a22-8e5f-8a77b1f9b2f4\",      \"created_at\": \"2025-10-29T14:12:01Z\",      \"updated_at\": \"2025-10-29T14:12:01Z\"    }</code></p> <p>Notes:    - <code>name</code> and <code>route</code> are required fields    - <code>route</code> must be unique and cannot be changed after creation    - <code>metadata</code> must be a valid JSON object    - The <code>id</code> is automatically generated if not provided    - Save the <code>id</code> value as you'll need it for all subsequent API requests</p> <ol> <li>Verify the installation</li> </ol> <p>List available models (replace <code>YOUR_DOMAIN_ID</code> with the domain ID from step 4):</p> <p><code>bash    curl -k https://localhost/proxy/YOUR_DOMAIN_ID/api/tags \\      -H \"Authorization: Bearer YOUR_ACCESS_TOKEN\"</code></p> <ol> <li>Make your first AI request</li> </ol> <p>Replace <code>YOUR_DOMAIN_ID</code> with your actual domain ID:</p> <p><code>bash    curl -k https://localhost/proxy/YOUR_DOMAIN_ID/v1/chat/completions \\      -H \"Content-Type: application/json\" \\      -H \"Authorization: Bearer YOUR_ACCESS_TOKEN\" \\      -d '{        \"model\": \"tinyllama:1.1b\",        \"messages\": [          {            \"role\": \"user\",            \"content\": \"Hello! How can you help me today?\"          }        ]      }'</code></p>"},{"location":"getting-started/#usage-guide","title":"Usage Guide","text":"<p>The following sections describe how to manage users and integrate with development tools.</p>"},{"location":"getting-started/#administrator-and-user-accounts","title":"Administrator and User Accounts","text":""},{"location":"getting-started/#administrator-access","title":"Administrator Access","text":"<p>If you are the instance administrator, you do not need to create a separate account for yourself. The platform is preconfigured with an administrator account that you can log into directly using your admin credentials. As the administrator, you can also generate authentication tokens for API access. Non-admin users, however, will need accounts created for them by the administrator.</p>"},{"location":"getting-started/#creating-a-new-user-account","title":"Creating a New User Account","text":"<p>As an administrator, you have the ability to create accounts for non-admin users and grant them access to Cube AI. Follow this demonstration to see the process in action. Here\u2019s a summary of the steps:</p> <ol> <li>Log in using your administrator credentials.</li> <li>Create a new domain (if one is needed).</li> <li>Log in to the newly created domain (or an existing domain).</li> <li>Click on your profile icon and select <code>Manage Users</code>.</li> <li>Click <code>Create</code> to start creating a new user.</li> <li>Fill out the user details in the form.</li> <li>Click <code>Create</code> to finalize the user creation.</li> <li>Share the username and password with the newly created user so they can log in.</li> </ol>"},{"location":"getting-started/#non-admin-user-login","title":"Non-Admin User Login","text":"<p>Once the administrator has created your account and shared the login details with you, use those credentials to log in to Cube AI. After logging in, you can obtain an authentication token for API interactions as shown below:</p> <pre><code>curl -ksSiX POST https://&lt;cube-ai-instance&gt;/users/tokens/issue -H \"Content-Type: application/json\" -d @- &lt;&lt; EOF\n{\n  \"username\": \"&lt;your_email&gt;\",\n  \"password\": \"&lt;your_password&gt;\"\n}\nEOF\n</code></pre> <p>Replace <code>&lt;your_email&gt;</code> and <code>&lt;your_password&gt;</code> with the credentials provided by the administrator.</p> <p>You will receive a response similar to the following:</p> <pre><code>HTTP/2 201\ncontent-type: application/json\ndate: Wed, 18 Sep 2024 11:13:48 GMT\nx-frame-options: DENY\nx-xss-protection: 1; mode=block\ncontent-length: 591\n\n{\"access_token\":\"&lt;access_token&gt;\",\"refresh_token\":\"&lt;refresh_token&gt;\"}\n</code></pre> <p>The <code>access_token</code> field contains your API token, which is required for making authenticated API calls. The <code>refresh_token</code> can be used to obtain a new access token when the current one expires.</p>"},{"location":"getting-started/#setting-up-vs-code-for-cube-ai-integration","title":"Setting Up VS Code for Cube AI Integration","text":"<p>To maximize Cube AI\u2019s potential within your development environment, you\u2019ll need to integrate it with Visual Studio Code (VS Code) using the Continue extension. This extension enables you to directly interact with LLMs in TEE inside VS Code, providing intelligent code completion, code suggestions, and contextual insights.</p>"},{"location":"getting-started/#steps-for-setting-up","title":"Steps for Setting Up","text":"<ol> <li>Download and install Visual Studio Code (VS Code).</li> <li>In VS Code, download and install the Continue extension, which connects Cube AI models to your development environment for enhanced coding assistance.</li> <li>Open the Continue extension by clicking the settings icon (gear icon), then select <code>Configure Continue</code>. This will open the <code>.continue/config.json</code> file. Alternatively:</li> <li>You can navigate to the <code>.continue</code> folder in your project\u2019s root directory using File Explorer.</li> <li>Press <code>Ctrl+Shift+P</code> to open the Command Palette and search for \"Continue: Open config.json\".</li> <li>Edit the <code>.continue/config.json</code> file to include the following configuration:</li> </ol> <pre><code>name: Local Assistant\nversion: 1.0.0\nschema: v1\nmodels:\n  - name: tinyllama\n    provider: ollama\n    model: tinyllama:1.1b\n    apiKey: &lt;access_token&gt;\n    apiBase: https://&lt;your-cube-instance&gt;/proxy/&lt;your-domain-id&gt;\n    requestOptions:\n      verifySsl: false\n  - name: Starcoder 2 3b\n    provider: ollama\n    model: starcoder2:7b\n    apiKey: &lt;access_token&gt;\n    apiBase: https://&lt;your-cube-instance&gt;/proxy/&lt;your-domain-id&gt;\n    requestOptions:\n      verifySsl: false\n  - name: Nomic Embed Text\n    provider: ollama\n    model: nomic-embed-text\n    apiKey: &lt;access_token&gt;\n    apiBase: https://&lt;your-cube-instance&gt;/proxy/&lt;your-domain-id&gt;\n    requestOptions:\n      verifySsl: false\ncontext:\n  - provider: code\n  - provider: docs\n  - provider: diff\n  - provider: terminal\n  - provider: problems\n  - provider: folder\n  - provider: codebase\n</code></pre> <p>Update the <code>apiKey</code> with your <code>access token</code>, the <code>apiBase</code> with the URL of your Cube AI instance, and replace <code>&lt;your-domain-id&gt;</code> with the domain ID from step 4. These values should reflect the actual deployment settings you're working with.</p> <p>For a more detailed explanation of how to connect to Cube AI with the continue extension, check out this video demonstration.</p>"},{"location":"opencode/","title":"Using OpenCode with Cube AI","text":"<p>This guide explains how to configure OpenCode to work with your Cube AI instance. OpenCode is an AI-powered code editor that can leverage the models hosted on your Cube AI platform.</p>"},{"location":"opencode/#prerequisites","title":"Prerequisites","text":"<ul> <li>A running Cube AI instance.</li> <li>OpenCode installed on your local machine.</li> <li>An active Cube AI user account and authentication token.</li> </ul>"},{"location":"opencode/#configuration","title":"Configuration","text":"<p>To connect OpenCode to Cube AI, you need to configure it to use the OpenAI-compatible API provided by the Cube AI proxy.</p>"},{"location":"opencode/#1-get-your-authentication-token","title":"1. Get your Authentication Token","text":"<p>If you haven't already, obtain your JWT token from the Cube AI instance:</p> <pre><code>curl -ksSiX POST https://&lt;cube-ai-instance&gt;/users/tokens/issue \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"username\": \"&lt;your_email&gt;\",\n    \"password\": \"&lt;your_password&gt;\"\n  }'\n</code></pre> <p>Copy the <code>access_token</code> from the response.</p>"},{"location":"opencode/#2-pulling-models","title":"2. Pulling Models","text":"<p>Before configuring OpenCode, ensure you have the appropriate models available in your Cube AI instance. For coding tasks, the <code>qwen2.5-coder</code> models are highly recommended.</p> <p>You can pull models using the Cube AI proxy API. You will need your authentication token and domain ID for this. Replace <code>&lt;your_domain_id&gt;</code> with the domain ID obtained from the Cube AI UI or created via the API (see the Getting Started Guide).</p> <pre><code># Pull Qwen 2.5 Coder 7B (Recommended)\ncurl -k -X POST https://&lt;cube-ai-instance&gt;/proxy/&lt;your_domain_id&gt;/api/pull \\\n  -H \"Authorization: Bearer &lt;your_access_token&gt;\" \\\n  -d '{\n    \"name\": \"qwen2.5-coder:7b-instruct-q4_K_M\"\n  }'\n</code></pre> <p>Other recommended models:</p> <ul> <li><code>qwen2.5-coder:3b</code> (Faster)</li> <li><code>deepseek-coder:6.7b-instruct-q4_K_M</code></li> </ul>"},{"location":"opencode/#3-verify-pulled-models","title":"3. Verify Pulled Models","text":"<p>You can verify that the models have been pulled by listing them:</p> <pre><code># List available models\ncurl -k https://&lt;cube-ai-instance&gt;/proxy/&lt;your_domain_id&gt;/v1/models \\\n  -H \"Authorization: Bearer &lt;your_access_token&gt;\"\n</code></pre>"},{"location":"opencode/#4-increasing-context-window-for-tool-support","title":"4. Increasing Context Window for Tool Support","text":"<p>Opencode requires larger context windows for tool functionality. Let's configure it to use a larger context window.</p> <pre><code># First, load the model\ncurl -k -X POST https://&lt;cube-ai-instance&gt;/proxy/&lt;your_domain_id&gt;/api/generate \\\n  -H \"Authorization: Bearer &lt;your_access_token&gt;\" \\\n  -d '{\n    \"model\": \"qwen2.5-coder:7b-instruct-q4_K_M\",\n    \"prompt\": \"test\",\n    \"stream\": false\n}'\n\n# Create a new model with increased context window\ncurl -k -X POST https://&lt;cube-ai-instance&gt;/proxy/&lt;your_domain_id&gt;/api/create \\\n  -H \"Authorization: Bearer &lt;your_access_token&gt;\" \\\n  -d '{\n    \"name\": \"qwen2.5-coder:7b-16k\",\n  \"modelfile\": \"FROM qwen2.5-coder:7b-instruct-q4_K_M\\nPARAMETER num_ctx 16384\"\n  }'\n</code></pre>"},{"location":"opencode/#5-verify-context-window-increase","title":"5. Verify Context Window Increase","text":"<pre><code># Check model info\ncurl -k https://&lt;cube-ai-instance&gt;/proxy/&lt;your_domain_id&gt;/api/show \\\n  -H \"Authorization: Bearer &lt;your_access_token&gt;\" \\\n  -d '{\"name\": \"qwen2.5-coder:7b-16k\"}'\n</code></pre>"},{"location":"opencode/#6-install-opencode","title":"6. Install OpenCode","text":"<p>You can install OpenCode from the install guide.</p>"},{"location":"opencode/#7-configure-opencode","title":"7. Configure OpenCode","text":"<p>You can configure OpenCode by editing the <code>~/.config/opencode/opencode.json</code> file. Create the file if it doesn't exist.</p> <p>Add the following configuration, replacing <code>&lt;cube-ai-instance&gt;</code> with your instance's address, <code>&lt;your_access_token&gt;</code> with the token you obtained, and <code>&lt;your_domain_id&gt;</code> with your domain ID:</p> <pre><code>{\n    \"$schema\": \"https://opencode.ai/config.json\",\n    \"model\": \"cube/qwen2.5-coder:7b-16k\",\n    \"provider\": {\n        \"cube\": {\n            \"npm\": \"@ai-sdk/openai-compatible\",\n            \"name\": \"Cube AI\",\n            \"options\": {\n                \"baseURL\": \"http://&lt;cube-ai-instance&gt;/proxy/&lt;your_domain_id&gt;/v1\",\n                \"apiKey\": \"&lt;your_access_token&gt;\"\n            },\n            \"models\": {\n                \"qwen2.5-coder:7b-16k\": {\n                    \"name\": \"Qwen 2.5 Coder 7B (16K) - Main\",\n                    \"tool_call\": false,\n                    \"reasoning\": false\n                },\n            },\n        }\n    }\n}\n</code></pre>"},{"location":"opencode/#8-verify-connection","title":"8. Verify Connection","text":"<ol> <li>Open OpenCode.</li> <li>Run <code>opencode /models</code> to see if the Cube AI models are listed.</li> </ol> <ol> <li>Select a model (e.g., <code>cube/qwen2.5-coder:7b-16k</code>).</li> <li>Try a simple prompt to verify the connection.</li> </ol>"},{"location":"opencode/#supported-features","title":"Supported Features","text":"<ul> <li>Code Generation: Use models like <code>qwen2.5-coder:7b-16k</code> for code completion and generation.</li> <li>Chat: Interact with models for general questions and assistance.</li> <li>Tools: If the model supports it (like <code>qwen2.5-coder:7b-16k</code> in the example), you can enable tools for file operations and command execution.</li> </ul>"},{"location":"opencode/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Connection Refused: Ensure your Cube AI instance is running and accessible from your machine.</li> <li>Authentication Error: Verify your JWT token is valid and correctly placed in the <code>Authorization</code> header.</li> <li>Certificate Errors: If using self-signed certificates, ensure <code>rejectUnauthorized</code> is set to <code>false</code> in the <code>tls</code> options.</li> </ul>"}]}