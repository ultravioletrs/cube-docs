{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Cube AI","text":"<p>Cube AI is a framework for building GPT-based AI applications using confidential computing. It protects user data and the AI model by using a trusted execution environment (TEE). TEE is a secure area of a processor that ensures that code and data loaded inside it are protected with respect to confidentiality and integrity. Data confidentiality prevents unauthorized access of data from outside the TEE, while code integrity ensures that code inside the TEE remains unchanged and unaltered from unauthorized access.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Secure Computing: Cube AI uses secure enclaves to protect user data and AI models from unauthorized access.</li> <li>Trusted Execution Environment (TEE): Cube AI uses a trusted execution environment to ensure that AI models are executed securely and in a controlled environment.</li> <li>Scalability: Cube AI can handle large amounts of data and AI models, making it suitable for applications that require high performance and scalability.</li> </ul>"},{"location":"#why-cube-ai","title":"Why Cube AI?","text":"<p>Traditional GPT-based AI applications often rely on public cloud services, which can be vulnerable to security breaches and unauthorized access. The tenant for example openai, and the hardware provider for example Azure, are not always transparent about their security practices and can be easily compromised. They can also access your prompts and model responses. Cube AI addresses these privacy concerns by using TEEs. Using TEEs, Cube AI ensures that user data and AI models are protected from unauthorized access outside the TEE. This helps to maintain user privacy and ensures that AI models are used in a controlled and secure manner.</p>"},{"location":"#how-does-cube-ai-work","title":"How does Cube AI work?","text":"<p>Cube AI uses TEEs to protect user data and AI models from unauthorized access. TEE offers an execution space that provides a higher level of security for trusted applications running on the device. In Cube AI, the TEE ensures that AI models are executed securely and in a controlled environment.</p>"},{"location":"architecture/","title":"Architecture","text":"<p>Cube AI is built on top of SuperMQ, Ollama and a custom proxy server. All these host the Cube AI API that hosts the AI models and protects prompts and user data securely.</p> <p></p>"},{"location":"architecture/#supermq","title":"SuperMQ","text":"<p>Cube AI uses SuperMQ Users and Auth Service to manage users and authentication. SuperMQ is an IoT platform that provides a secure and scalable way to manage IoT devices. Since SuperMQ is based on micro-service architecture, auth and users are separated from the core application. This allows for better scalability and security. SuperMQ is responsible for users authentication, authorization and user management. It also provides a secure way to store user data.</p>"},{"location":"architecture/#ollama","title":"Ollama","text":"<p>Ollama is a framework that provides a unified interface for interacting with different LLMs. Ollama is responsible for managing LLMs and their configurations. It provides a unified interface for interacting with different LLMs, allowing developers to easily switch between different LLMs without having to change their code. Ollama also provides a way to manage LLMs, including configuring LLMs, managing prompts, and managing models.</p>"},{"location":"architecture/#proxy-server","title":"Proxy Server","text":"<p>The proxy server is responsible for handling requests to ollama. Once a user is registered on SuperMQ and issued with an access token, the user can interact with Cube AI using the issued token. The proxy server will verify the token and ensure that the user has the necessary permissions to access the Cube AI API by checking it with the SuperMQ Auth Service. Once the request is verified, the proxy server will forward the request to the appropriate Ollama instance. The proxy server also handles authentication and authorization for the user. It ensures that only authorized users can access the Cube AI API.</p>"},{"location":"attestation/","title":"Attestation","text":"<p>Attestation is a process of verifying the integrity and authenticity of software or hardware components. In the context of Cube AI, attestation is used to verify the integrity of the Cube AI API and the Ollama instances.</p>"},{"location":"attestation/#runtime-encryption","title":"Runtime Encryption","text":"<p>Cube AI runs inside AMD SEV-SNP based confidential virtual machines (CVMs). The CVMs are secured using AMD SEV-SNP, which provides a secure enclave for running applications. With SEV-SNP, the memory of the CVM is encrypted, ensuring that the data is not accessible to the host operating system or other applications running on the host. This ensures that the data is not accessible to unauthorized users. The encryption keys are generated using a hardware-based key management system, ensuring that the keys are not accessible to unauthorized users. Even if an attacker gains access to the hypervisor, they will not be able to read the encrypted data or access the encryption keys.</p>"},{"location":"developer-guide/","title":"Developer Guide","text":"<p>This guide will help you get started with developing, testing, deploying, and running Cube AI.</p>"},{"location":"developer-guide/#cloning-the-repository","title":"Cloning the Repository","text":"<p>Clone Cube AI repository by running:</p> <pre><code>git clone https://github.com/ultravioletrs/cube.git\ncd cube\n</code></pre>"},{"location":"developer-guide/#pulling-docker-images","title":"Pulling Docker Images","text":"<p>Pull the Docker images by running:</p> <pre><code>cd cube/docker/\ndocker compose pull\n</code></pre>"},{"location":"developer-guide/#running-services-with-docker-compose","title":"Running Services with Docker Compose","text":"<p>If you are running Cube AI on your local machine:</p> <ol> <li>Update the Environment File    Open the <code>docker/.env</code> file and set the <code>UV_CUBE_NEXTAUTH_URL</code> to point to <code>localhost</code> using the specified UI port (default is <code>6193</code>):</li> </ol> <p><code>bash    UV_CUBE_NEXTAUTH_URL=http://localhost:${UI_PORT}</code></p> <ol> <li>Start the Docker Containers    Run the following command to start the services:</li> </ol> <p><code>bash    docker compose up -d</code></p> <ol> <li>Access the UI    Open your browser and navigate to:</li> </ol> <p><code>bash    http://localhost:6193</code></p> <p>If you are running Cube AI on a different server (e.g., a virtual machine or cloud instance):</p> <ol> <li>Update the Environment File     Open the <code>docker/.env</code> file and replace the placeholders with your server's IP address and UI port. For example, if your server's IP address is <code>209.18.187.53</code> and you are using the default port <code>6193</code>, update the file as follows:</li> </ol> <p><code>bash    UI_PORT=6193    UV_CUBE_NEXTAUTH_URL=http://209.18.187.53:${UI_PORT}</code></p> <p>You can set a custom port if <code>6193</code> is unavailable or conflicts with other services. For instance, if you want to use port <code>8080</code>, update the <code>docker/.env</code> file as follows:</p> <p><code>bash    UI_PORT=8080    UV_CUBE_NEXTAUTH_URL=http://209.18.187.53:${UI_PORT}</code></p> <p>Ensure the chosen port is open and not restricted by firewalls.</p> <ol> <li>Start the Docker Containers    Run the following command on the server:</li> </ol> <p><code>bash    docker compose up -d</code></p> <ol> <li> <p>Access the UI    Open your browser and navigate to:</p> </li> <li> <p>For the default port:</p> <p><code>bash  http://209.18.187.53:6193</code></p> </li> <li> <p>For a custom port (e.g., <code>8080</code>):</p> <p><code>bash  http://209.18.187.53:8080</code></p> </li> </ol> <p>After successfully deploying Cube AI, you can perform various tasks to set up and manage your instance. These include:</p> <ul> <li>Managing domains</li> <li>Managing users</li> <li>Generating authentication tokens (using curl command)</li> <li>Intergrating your <code>Cube AI</code> instance with <code>Continue</code> Visual Studio Code extension to leverage large language models (LLMs) directly within VS Code to assist with coding task</li> </ul> <p>For detailed instructions on the above tasks and more, refer to the Getting Started Guide.</p>"},{"location":"developer-guide/#open-web-ui-integration","title":"Open Web UI Integration","text":"<p>Open Web UI is integrated into Cube AI to help in debugging and monitoring key performance metrics of the LLMs, including response token/s, prompt token/s, total duration, and load duration. For more detailed setup and configuration instructions, refer to the Open Web UI documentation.</p> <p>To access Open Web UI, once Cube AI services are up and running, open your browser and navigate to:</p> <pre><code>http://&lt;your-server-ip-address&gt;:3000\n</code></pre> <p>While it should work out of the box, occasionally, when you submit a prompt through the Open Web UI, you might encounter an error like this:</p> <pre><code>Ollama: 400, message='Bad Request', url='http://ollama:11434/api/chat'\n</code></pre> <p>To resolve the error:</p> <ul> <li>Click on your profile icon in the top-right corner of the Open Web UI interface.</li> <li>Navigate to Settings.</li> <li>Select Admin Settings.</li> <li>In the Admin Panel, select Connections from the sidebar.</li> <li>Under the Ollama API section, click the refresh icon next to the Ollama API URL (<code>http://ollama:11434</code>).</li> <li>After refreshing, you should see a confirmation message stating \"Server connection verified\". This should reset the connection to the Ollama service and resolve the \"Bad Request\" error.</li> </ul>"},{"location":"developer-guide/#building-docker-images","title":"Building Docker Images","text":"<p>You can build the Docker images for Cube AI and related services using the <code>make</code> command in the project's root directory.</p> <p>To build the production Docker image, use:</p> <pre><code>make docker\n</code></pre> <p>For the development Docker image, use:</p> <pre><code>make docker-dev\n</code></pre>"},{"location":"developer-guide/#hardware-abstraction-layer-hal-for-confidential-computing","title":"Hardware Abstraction Layer (HAL) for Confidential Computing","text":"<p>For detailed instructions on setting up and building Cube HAL, please refer to this guide. It covers:</p> <ul> <li>Cloning the Buildroot and Cube repositories</li> <li>Configuring and building Cube HAL</li> <li>Running Cube HAL in a virtual machine</li> </ul>"},{"location":"developer-guide/#private-model-upload","title":"Private Model Upload","text":"<ol> <li>Package the Model Files</li> </ol> <p>First, compress your model files into a <code>.tar.gz</code> archive to prepare them for transfer:</p> <p><code>bash    tar -czvf model-name.tar.gz /path/to/model/files</code></p> <ol> <li>Set Up Cube HAL</li> </ol> <p>Follow the Hardware Abstraction Layer (HAL) for Confidential Computing setup to:</p> <ul> <li>Install Buildroot</li> <li>Clone the Cube AI repository</li> <li> <p>Pull and start Cube AI docker containers</p> </li> <li> <p>Enable SSH and Access the CVM</p> </li> </ul> <p>SSH is needed to securely connect and transfer files to the CVM. Follow the steps in the SSH guide to enable SSH access on the CVM.</p> <ol> <li>Transfer the Model Archive to the CVM</li> </ol> <p>Once SSH is set up, use <code>scp</code> to securely copy the <code>.tar.gz</code> file from step 1 to the CVM. For detailed steps, refer to the SSH guide.</p> <ol> <li>Decompress and Extract the Model Files on the CVM    After transferring the archive to the CVM, log in to the CVM and use <code>gunzip</code> to decompress the <code>.tar.gz</code> file, then extract the <code>.tar</code> file:</li> </ol> <p><code>bash    gunzip model-name.tar.gz    tar -xvf model-name.tar</code></p> <ol> <li>Copy the Extracted Model Files to the <code>ollama</code> Container</li> </ol> <p>With the model files unpacked, copy them to the <code>ollama</code> container running on the CVM:</p> <p><code>bash    docker cp /path/to/extracted/files ollama:/path/to/target/directory/in/container</code></p> <p>Replace <code>/path/to/extracted/files</code> with the path of the unpacked files on the CVM and <code>/path/to/target/directory/in/container</code> with the destination directory inside the <code>ollama</code> container.</p>"},{"location":"developer-guide/#fine-tuning-base-model-on-custom-code-dataset","title":"Fine-Tuning Base Model on Custom Code Dataset","text":"<p>To enhance the performance of the base model on domain-specific tasks or particular code styles, you may want to fine-tune it on a custom code dataset. This process will help adapt the model to your specific requirements.</p>"},{"location":"developer-guide/#prerequisites","title":"Prerequisites","text":"<ol> <li> <p>Python Environment: Ensure you have Python 3.8 or above.</p> </li> <li> <p>Dependencies:</p> </li> <li> <p><code>transformers</code>: for model handling. For more information, see the transformers Documentation.</p> </li> <li><code>datasets</code>: for data handling. For more information, see the datasets library.</li> <li><code>torch</code>: for model training. Alternative is <code>TensorFlow</code> with <code>Keras</code>. For more information, see the TensorFlow Documentation.</li> <li><code>peft</code>: for parameter-efficient fine-tuning (PEFT) if needed. For more information, see the PEFT Documentation.</li> <li><code>unsloth</code>: for advanced model handling and integration of LoRA adapters and other fine-tuning techniques. For more information, see the unsloth GitHub Repository.</li> <li>Optional Libraries: Install additional packages for LoRA fine-tuning, data handling, and model quantization on Google Colab. Execute the following in Colab:</li> </ol> <p><code>python    !pip install torch==2.5.0+cu121 -f https://download.pytorch.org/whl/cu121/torch_stable.html    !pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"    !pip install --no-deps xformers \"trl&lt;0.9.0\" peft accelerate bitsandbytes</code></p> <ol> <li> <p>Code Database: You can use a public code dataset from Hugging Face\u2019s datasets library or your own code dataset.</p> </li> <li> <p>Google Drive (Optional): Mount Google Drive if you're using Colab and need a place to store datasets and model outputs.</p> </li> </ol> <p><code>python    from google.colab import drive    drive.mount('/content/drive')</code></p> <ol> <li>Ollama: Install Ollama for model deployment.</li> </ol> <p><code>bash    !curl -fsSL https://ollama.com/install.sh | sh</code></p>"},{"location":"developer-guide/#step-1-prepare-your-code-dataset","title":"Step 1: Prepare Your Code Dataset","text":"<p>Organize your dataset into prompt-input-completion pairs:</p> <ul> <li>Prompt: The task or question, e.g., \"Write a function to reverse a string.\"</li> <li>Input: Additional context to guide the model, like sample data or requirements.</li> <li>Completion: The expected solution, such as the completed function code.</li> </ul> <p>This structure enables effective model learning by linking each prompt and input to the correct completion. For more details, refer to the TRL Dataset Formats Documentation.</p> <pre><code>from datasets import load_dataset\nfrom transformers import AutoTokenizer\n\n# Load your dataset - replace `your_code_dataset` with your dataset name or path to your file\n# Example: If using a custom CSV file, specify the path as shown below.\ndataset = load_dataset(\"csv\", data_files=\"path/to/your/data.csv\")\n\n# Initialize tokenizer (replace with your specific model)\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-Coder-1.5B\")\n\n# Define a structured template for formatting\nformatted_prompt_template = \"\"\"Below is an instruction, an input, and a response that completes the request.\n\n### Instruction:\n{}\n\n### Input:\n{}\n\n### Response:\n{}\"\"\"\n\n# Define a function to format each example in prompt-input-completion structure\ndef format_prompt_input_completion_pairs(examples):\n    combined_texts = [\n        formatted_prompt_template.format(prompt, input_text, completion) + tokenizer.eos_token\n        for prompt, input_text, completion in zip(examples[\"prompt\"], examples[\"input\"], examples[\"completion\"])\n    ]\n    return {\"text\": combined_texts}\n\n# Apply the formatting function to the dataset in batches for efficiency\nformatted_dataset = dataset.map(format_prompt_input_completion_pairs, batched=True)\n</code></pre> <p>If your code dataset has several files like in a github repository, first convert them into a single text file to streamline preprocessing. To do this, use Repomix. With the generated <code>repomix-output.txt</code>, create prompt-input-completion pairs by segmenting each function or code block in line with your chosen prompt.</p> <p>Save the formatted dataset if working on Google Colab with Google Drive:</p> <pre><code>output_file_path = '/content/drive/MyDrive/formatted_dataset.jsonl'\nwith open(output_file_path, 'w', encoding='utf-8') as output_file:\n    for entry in formatted_dataset:\n        json.dump({\"text\": entry[\"text\"]}, output_file)\n        output_file.write('\\n')\n</code></pre>"},{"location":"developer-guide/#step-3-add-lora-adapters-optional-for-parameter-efficient-fine-tuning","title":"Step 3: Add LoRA Adapters (Optional for Parameter-Efficient Fine-Tuning)","text":"<p>LoRA (Low-Rank Adaptation) adapters help reduce memory usage and improve training efficiency by modifying specific layers of the model.</p> <pre><code>from unsloth import FastLanguageModel\n\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r=16,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n    lora_alpha=8,\n    lora_dropout=0.1,\n    bias=\"none\",\n    use_gradient_checkpointing=\"unsloth\",\n    random_state=3407\n)\n</code></pre>"},{"location":"developer-guide/#step-4-define-training-arguments","title":"Step 4: Define Training Arguments","text":"<p>Set up your training arguments for fine-tuning. Adjust the following values based on the dataset size, available hardware, and specific requirements.</p> <pre><code>training_args = TrainingArguments(\n    output_dir=\"./qwen_coder_finetuned\",\n    evaluation_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2,\n    num_train_epochs=3,\n    weight_decay=0.01,\n    save_total_limit=2,\n    push_to_hub=False,  # Set to True if pushing to Hugging Face hub\n)\n</code></pre> <p>For PEFT models using LoRA, you might want to adjust batch sizes, accumulation steps, and number of epochs to better suit smaller datasets.</p> <pre><code>training_args = TrainingArguments(\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=8,\n    num_train_epochs=2,\n    learning_rate=5e-5,\n    fp16=not is_bfloat16_supported(),\n    bf16=is_bfloat16_supported(),\n    logging_steps=1,\n    optim=\"adamw_8bit\",\n    output_dir=\"outputs\",\n)\n</code></pre>"},{"location":"developer-guide/#step-5-initialize-the-sft-trainer","title":"Step 5: Initialize the SFT Trainer","text":"<p>Use the Hugging Face <code>Trainer</code> to manage the training process.</p> <pre><code>trainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"validation\"],\n)\n</code></pre> <p>For LoRA fine-tuning, use <code>SFTTrainer</code> instead:</p> <pre><code>from trl import SFTTrainer\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=formatted_dataset,\n    dataset_text_field=\"text\",\n    max_seq_length=2048,\n)\n</code></pre>"},{"location":"developer-guide/#step-6-train-the-model","title":"Step 6: Train the Model","text":"<p>Start the fine-tuning process. This may take some time depending on your dataset size and compute resources.</p> <pre><code>trainer.train()\n</code></pre>"},{"location":"developer-guide/#step-7-evaluate-the-model-optional","title":"Step 7: Evaluate the Model (Optional)","text":"<p>After training, evaluate the model on a test dataset to check its performance.</p> <pre><code>eval_results = trainer.evaluate()\nprint(f\"Evaluation results: {eval_results}\")\n</code></pre>"},{"location":"developer-guide/#step-8-save-and-export-the-model","title":"Step 8: Save and Export the Model","text":"<p>Save the fine-tuned model to the local disk, or optionally push it to the Hugging Face Hub.</p> <pre><code>trainer.save_model(\"path/to/save/qwen_coder_finetuned\")\n# Optionally, push to Hugging Face Hub if push_to_hub=True in TrainingArguments\n# trainer.push_to_hub()\n</code></pre>"},{"location":"developer-guide/#step-9-convert-to-gguf-format-using-llamacpp","title":"Step 9: Convert to GGUF Format Using <code>llama.cpp</code>","text":"<p>To make the fine-tuned model deployable on a variety of platforms, you can convert it to GGUF formats using <code>llama.cpp</code>.</p> <pre><code>    %cd llama.cpp\n    !make\n    model.push_to_hub_gguf(\n        \"hf_username/model_name\",\n        tokenizer=tokenizer,\n        quantization_method=[\"q4_k_m\", \"q5_k_m\", \"q8_0\", \"f16\"],\n        token=\"hf_token\"\n    )\n</code></pre> <p>For a complete notebook fine-tuning Qwen 2.5 Coder 1.5B, refer to Fine-Tuning Qwen 2.5 Coder 1.5B</p>"},{"location":"developer-guide/#cleaning-up-your-dockerized-cube-ai-setup","title":"Cleaning up your Dockerized Cube AI Setup","text":"<p>If you want to stop and remove the Cube AI services, volumes, and networks created during the setup, follow these steps:</p> <p>First, stop all running containers:</p> <pre><code>docker compose down\n</code></pre> <p>Remove volumes and networks:</p> <pre><code>docker compose down --volumes --remove-orphans\n</code></pre> <p>To clean up the build artifacts and remove compiled files, use:</p> <pre><code>make clean\n</code></pre>"},{"location":"getting-started/","title":"Getting Started","text":"<p>Before logging in or interacting with Cube AI, ensure that Cube AI has been properly deployed and is running. Deployment can be performed by the instance administrator or by you, provided you have the necessary access permissions. For a detailed guide on how to deploy Cube AI, refer to the developer guide.</p> <p>Therefore, to connect:</p> <ol> <li>Ensure that Cube AI is deployed and running on your instance.</li> <li>Connect to the deployed Cube AI instance on port <code>6193</code> using your login credentials (username and password).</li> </ol> <p>Port <code>6193</code> is the default port for accessing Cube AI deployment. It is configurable through the <code>docker/.env</code> file, meaning you can change it to any port that suits your deployment needs as described in the developer guide.</p>"},{"location":"getting-started/#administrator-and-user-accounts","title":"Administrator and User Accounts","text":""},{"location":"getting-started/#administrator-access","title":"Administrator Access","text":"<p>If you are the instance administrator, you do not need to create a separate account for yourself. The platform is preconfigured with an administrator account that you can log into directly using your admin credentials. As the administrator, you can also generate authentication tokens for API access. Non-admin users, however, will need accounts created for them by the administrator.</p>"},{"location":"getting-started/#creating-a-new-user-account","title":"Creating a New User Account","text":"<p>As an administrator, you have the ability to create accounts for non-admin users and grant them access to Cube AI. Follow this demonstration to see the process in action. Here\u2019s a summary of the steps:</p> <ol> <li>Log in using your administrator credentials.</li> <li>Create a new domain (if one is needed).</li> <li>Log in to the newly created domain (or an existing domain).</li> <li>Click on your profile icon and select <code>Manage Users</code>.</li> <li>Click <code>Create</code> to start creating a new user.</li> <li>Fill out the user details in the form.</li> <li>Click <code>Create</code> to finalize the user creation.</li> <li>Share the username and password with the newly created user so they can log in.</li> </ol>"},{"location":"getting-started/#non-admin-user-login","title":"Non-Admin User Login","text":"<p>Once the administrator has created your account and shared the login details with you, use those credentials to log in to Cube AI. After logging in, you can obtain an authentication token for API interactions as shown below:</p> <pre><code>curl -ksSiX POST https://&lt;cube-ai-instance&gt;/users/tokens/issue -H \"Content-Type: application/json\" -d @- &lt;&lt; EOF\n{\n  \"identity\": \"&lt;your_email&gt;\",\n  \"secret\": \"&lt;your_password&gt;\"\n}\nEOF\n</code></pre> <p>Replace <code>&lt;your_email&gt;</code> and <code>&lt;your_password&gt;</code> with the credentials provided by the administrator.</p> <p>You will receive a response similar to the following:</p> <pre><code>HTTP/2 201\ncontent-type: application/json\ndate: Wed, 18 Sep 2024 11:13:48 GMT\nx-frame-options: DENY\nx-xss-protection: 1; mode=block\ncontent-length: 591\n\n{\"access_token\":\"&lt;access_token&gt;\",\"refresh_token\":\"&lt;refresh_token&gt;\"}\n</code></pre> <p>The <code>access_token</code> field contains your API token, which is required for making authenticated API calls. The <code>refresh_token</code> can be used to obtain a new access token when the current one expires.</p>"},{"location":"getting-started/#setting-up-vs-code-for-cube-ai-integration","title":"Setting Up VS Code for Cube AI Integration","text":"<p>To maximize Cube AI\u2019s potential within your development environment, you\u2019ll need to integrate it with Visual Studio Code (VS Code) using the Continue extension. This extension enables you to directly interact with LLMs in TEE inside VS Code, providing intelligent code completion, code suggestions, and contextual insights.</p>"},{"location":"getting-started/#steps-for-setting-up","title":"Steps for Setting Up","text":"<ol> <li>Download and install Visual Studio Code (VS Code).</li> <li>In VS Code, download and install the Continue extension, which connects Cube AI models to your development environment for enhanced coding assistance.</li> <li>Open the Continue extension by clicking the settings icon (gear icon), then select <code>Configure Continue</code>. This will open the <code>.continue/config.json</code> file. Alternatively:</li> <li>You can navigate to the <code>.continue</code> folder in your project\u2019s root directory using File Explorer.</li> <li>Press <code>Ctrl+Shift+P</code> to open the Command Palette and search for \"Continue: Open config.json\".</li> <li>Edit the <code>.continue/config.json</code> file to include the following configuration:</li> </ol> <pre><code>{\n  \"models\": [\n    {\n      \"title\": \"tinyllama\",\n      \"provider\": \"ollama\",\n      \"model\": \"tinyllama:1.1b\",\n      \"apiKey\": \"&lt;access_token&gt;\",\n      \"apiBase\": \"http://&lt;your-ollama-instance&gt;/ollama\"\n    }\n  ],\n  \"tabAutocompleteModel\": {\n    \"title\": \"Starcoder 2 3b\",\n    \"provider\": \"ollama\",\n    \"model\": \"starcoder2:7b\",\n    \"apiKey\": \"&lt;access_token&gt;\",\n    \"apiBase\": \"http://&lt;your-ollama-instance&gt;/ollama\"\n  },\n  \"embeddingsProvider\": {\n    \"provider\": \"ollama\",\n    \"model\": \"nomic-embed-text\",\n    \"apiKey\": \"&lt;access_token&gt;\",\n    \"apiBase\": \"http://&lt;your-ollama-instance&gt;/ollama\"\n  },\n  \"requestOptions\": {\n    \"verifySsl\": false\n  },\n  \"customCommands\": [\n    {\n      \"name\": \"test\",\n      \"prompt\": \"{{{ input }}}\\n\\nWrite a comprehensive set of unit tests for the selected code. It should setup, run tests that check for correctness including important edge cases, and teardown. Ensure that the tests are complete and sophisticated. Give the tests just as chat output, don't edit any file.\",\n      \"description\": \"Write unit tests for highlighted code\"\n    }\n  ],\n  \"contextProviders\": [\n    {\n      \"name\": \"code\",\n      \"params\": {}\n    },\n    {\n      \"name\": \"docs\",\n      \"params\": {}\n    },\n    {\n      \"name\": \"diff\",\n      \"params\": {}\n    },\n    {\n      \"name\": \"terminal\",\n      \"params\": {}\n    },\n    {\n      \"name\": \"problems\",\n      \"params\": {}\n    },\n    {\n      \"name\": \"folder\",\n      \"params\": {}\n    },\n    {\n      \"name\": \"codebase\",\n      \"params\": {}\n    }\n  ],\n  \"slashCommands\": [\n    {\n      \"name\": \"edit\",\n      \"description\": \"Edit selected code\"\n    },\n    {\n      \"name\": \"comment\",\n      \"description\": \"Write comments for the selected code\"\n    },\n    {\n      \"name\": \"share\",\n      \"description\": \"Export the current chat session to markdown\"\n    },\n    {\n      \"name\": \"cmd\",\n      \"description\": \"Generate a shell command\"\n    },\n    {\n      \"name\": \"commit\",\n      \"description\": \"Generate a git commit message\"\n    }\n  ]\n}\n</code></pre> <p>Update the <code>apiKey</code> with your <code>access token</code> and the <code>apiBase</code> with the URL of your Cube AI instance (if different from the default one). These values should reflect the actual deployment settings you're working with.</p> <p>For a more detailed explanation of how to connect to Cube AI with the continue extension, check out this video demonstration.</p>"}]}